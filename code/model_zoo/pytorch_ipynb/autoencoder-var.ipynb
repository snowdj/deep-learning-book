{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 7.2.0\n",
      "\n",
      "torch 0.4.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runs on CPU or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple variational autoencoder that compresses 768-pixel MNIST images down to a 15-pixel latent vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 0\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_hidden_1 = 500\n",
    "num_latent = 15\n",
    "\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden_1, num_latent):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        ### ENCODER\n",
    "        self.hidden_1 = torch.nn.Linear(num_features, num_hidden_1)\n",
    "        self.z_mean = torch.nn.Linear(num_hidden_1, num_latent)\n",
    "        # in the original paper (Kingma & Welling 2015, we use\n",
    "        # have a z_mean and z_var, but the problem is that\n",
    "        # the z_var can be negative, which would cause issues\n",
    "        # in the log later. Hence we assume that latent vector\n",
    "        # has a z_mean and z_log_var component, and when we need\n",
    "        # the regular variance or std_dev, we simply use \n",
    "        # an exponential function\n",
    "        self.z_log_var = torch.nn.Linear(num_hidden_1, num_latent)\n",
    "        \n",
    "        \n",
    "        ### DECODER\n",
    "        self.linear_3 = torch.nn.Linear(num_latent, num_hidden_1)\n",
    "        self.linear_4 = torch.nn.Linear(num_hidden_1, num_features)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        # Sample epsilon from standard normal distribution\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(device)\n",
    "        # note that log(x^2) = 2*log(x); hence divide by 2 to get std_dev\n",
    "        # i.e., std_dev = exp(log(std_dev^2)/2) = exp(log(var)/2)\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.) \n",
    "        return z\n",
    "        \n",
    "    def encoder(self, features):\n",
    "        x = self.hidden_1(features)\n",
    "        x = F.leaky_relu(x, negative_slope=0.0001)\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_var = self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        return z_mean, z_log_var, encoded\n",
    "    \n",
    "    def decoder(self, encoded):\n",
    "        x = self.linear_3(encoded)\n",
    "        x = F.leaky_relu(x, negative_slope=0.0001)\n",
    "        x = self.linear_4(x)\n",
    "        decoded = torch.sigmoid(x)\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, features):\n",
    "        \n",
    "        z_mean, z_log_var, encoded = self.encoder(features)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return z_mean, z_log_var, encoded, decoded\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = VariationalAutoencoder(num_features,\n",
    "                               num_hidden_1,\n",
    "                               num_latent)\n",
    "model = model.to(device)\n",
    "    \n",
    "\n",
    "##########################\n",
    "### COST AND OPTIMIZER\n",
    "##########################\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 000/468 | Cost: 70481.2422\n",
      "Epoch: 001/050 | Batch 050/468 | Cost: 27139.5547\n",
      "Epoch: 001/050 | Batch 100/468 | Cost: 22833.3730\n",
      "Epoch: 001/050 | Batch 150/468 | Cost: 19493.1523\n",
      "Epoch: 001/050 | Batch 200/468 | Cost: 18727.4688\n",
      "Epoch: 001/050 | Batch 250/468 | Cost: 18074.2676\n",
      "Epoch: 001/050 | Batch 300/468 | Cost: 16633.2852\n",
      "Epoch: 001/050 | Batch 350/468 | Cost: 17138.0371\n",
      "Epoch: 001/050 | Batch 400/468 | Cost: 16400.9316\n",
      "Epoch: 001/050 | Batch 450/468 | Cost: 16061.1406\n",
      "Epoch: 002/050 | Batch 000/468 | Cost: 16585.8066\n",
      "Epoch: 002/050 | Batch 050/468 | Cost: 15458.3271\n",
      "Epoch: 002/050 | Batch 100/468 | Cost: 15661.5742\n",
      "Epoch: 002/050 | Batch 150/468 | Cost: 15742.9580\n",
      "Epoch: 002/050 | Batch 200/468 | Cost: 15151.9629\n",
      "Epoch: 002/050 | Batch 250/468 | Cost: 15335.2969\n",
      "Epoch: 002/050 | Batch 300/468 | Cost: 15412.3613\n",
      "Epoch: 002/050 | Batch 350/468 | Cost: 15649.0166\n",
      "Epoch: 002/050 | Batch 400/468 | Cost: 14799.8145\n",
      "Epoch: 002/050 | Batch 450/468 | Cost: 15034.5771\n",
      "Epoch: 003/050 | Batch 000/468 | Cost: 14455.5244\n",
      "Epoch: 003/050 | Batch 050/468 | Cost: 14487.1230\n",
      "Epoch: 003/050 | Batch 100/468 | Cost: 14379.7812\n",
      "Epoch: 003/050 | Batch 150/468 | Cost: 13937.0381\n",
      "Epoch: 003/050 | Batch 200/468 | Cost: 15056.1543\n",
      "Epoch: 003/050 | Batch 250/468 | Cost: 14659.6074\n",
      "Epoch: 003/050 | Batch 300/468 | Cost: 14310.2529\n",
      "Epoch: 003/050 | Batch 350/468 | Cost: 14326.4375\n",
      "Epoch: 003/050 | Batch 400/468 | Cost: 14493.2002\n",
      "Epoch: 003/050 | Batch 450/468 | Cost: 13755.8633\n",
      "Epoch: 004/050 | Batch 000/468 | Cost: 15022.6123\n",
      "Epoch: 004/050 | Batch 050/468 | Cost: 14308.9424\n",
      "Epoch: 004/050 | Batch 100/468 | Cost: 14729.3496\n",
      "Epoch: 004/050 | Batch 150/468 | Cost: 15450.3643\n",
      "Epoch: 004/050 | Batch 200/468 | Cost: 14125.2559\n",
      "Epoch: 004/050 | Batch 250/468 | Cost: 14322.2441\n",
      "Epoch: 004/050 | Batch 300/468 | Cost: 13802.1514\n",
      "Epoch: 004/050 | Batch 350/468 | Cost: 14325.1436\n",
      "Epoch: 004/050 | Batch 400/468 | Cost: 14126.1562\n",
      "Epoch: 004/050 | Batch 450/468 | Cost: 13605.7207\n",
      "Epoch: 005/050 | Batch 000/468 | Cost: 14003.4014\n",
      "Epoch: 005/050 | Batch 050/468 | Cost: 14229.5156\n",
      "Epoch: 005/050 | Batch 100/468 | Cost: 13989.3262\n",
      "Epoch: 005/050 | Batch 150/468 | Cost: 13905.5205\n",
      "Epoch: 005/050 | Batch 200/468 | Cost: 13811.3242\n",
      "Epoch: 005/050 | Batch 250/468 | Cost: 14404.5801\n",
      "Epoch: 005/050 | Batch 300/468 | Cost: 14600.5820\n",
      "Epoch: 005/050 | Batch 350/468 | Cost: 13643.7363\n",
      "Epoch: 005/050 | Batch 400/468 | Cost: 13638.7588\n",
      "Epoch: 005/050 | Batch 450/468 | Cost: 13628.8398\n",
      "Epoch: 006/050 | Batch 000/468 | Cost: 13980.3691\n",
      "Epoch: 006/050 | Batch 050/468 | Cost: 13947.5898\n",
      "Epoch: 006/050 | Batch 100/468 | Cost: 14273.9102\n",
      "Epoch: 006/050 | Batch 150/468 | Cost: 13402.6943\n",
      "Epoch: 006/050 | Batch 200/468 | Cost: 13806.1553\n",
      "Epoch: 006/050 | Batch 250/468 | Cost: 14134.9971\n",
      "Epoch: 006/050 | Batch 300/468 | Cost: 12889.6660\n",
      "Epoch: 006/050 | Batch 350/468 | Cost: 13456.7979\n",
      "Epoch: 006/050 | Batch 400/468 | Cost: 13381.0176\n",
      "Epoch: 006/050 | Batch 450/468 | Cost: 13533.8623\n",
      "Epoch: 007/050 | Batch 000/468 | Cost: 13898.8125\n",
      "Epoch: 007/050 | Batch 050/468 | Cost: 13676.7910\n",
      "Epoch: 007/050 | Batch 100/468 | Cost: 14121.5400\n",
      "Epoch: 007/050 | Batch 150/468 | Cost: 14050.7598\n",
      "Epoch: 007/050 | Batch 200/468 | Cost: 13711.5225\n",
      "Epoch: 007/050 | Batch 250/468 | Cost: 14280.2617\n",
      "Epoch: 007/050 | Batch 300/468 | Cost: 13202.4414\n",
      "Epoch: 007/050 | Batch 350/468 | Cost: 14144.4648\n",
      "Epoch: 007/050 | Batch 400/468 | Cost: 13782.0527\n",
      "Epoch: 007/050 | Batch 450/468 | Cost: 13933.5498\n",
      "Epoch: 008/050 | Batch 000/468 | Cost: 13544.2812\n",
      "Epoch: 008/050 | Batch 050/468 | Cost: 13570.6318\n",
      "Epoch: 008/050 | Batch 100/468 | Cost: 13871.6777\n",
      "Epoch: 008/050 | Batch 150/468 | Cost: 13134.0889\n",
      "Epoch: 008/050 | Batch 200/468 | Cost: 13159.7168\n",
      "Epoch: 008/050 | Batch 250/468 | Cost: 13501.7002\n",
      "Epoch: 008/050 | Batch 300/468 | Cost: 13692.8496\n",
      "Epoch: 008/050 | Batch 350/468 | Cost: 13922.7695\n",
      "Epoch: 008/050 | Batch 400/468 | Cost: 14085.8184\n",
      "Epoch: 008/050 | Batch 450/468 | Cost: 13280.8057\n",
      "Epoch: 009/050 | Batch 000/468 | Cost: 13640.2354\n",
      "Epoch: 009/050 | Batch 050/468 | Cost: 13840.1602\n",
      "Epoch: 009/050 | Batch 100/468 | Cost: 13847.2744\n",
      "Epoch: 009/050 | Batch 150/468 | Cost: 14598.1777\n",
      "Epoch: 009/050 | Batch 200/468 | Cost: 13850.5020\n",
      "Epoch: 009/050 | Batch 250/468 | Cost: 13663.8223\n",
      "Epoch: 009/050 | Batch 300/468 | Cost: 13657.1875\n",
      "Epoch: 009/050 | Batch 350/468 | Cost: 13415.3438\n",
      "Epoch: 009/050 | Batch 400/468 | Cost: 14466.6016\n",
      "Epoch: 009/050 | Batch 450/468 | Cost: 13589.5068\n",
      "Epoch: 010/050 | Batch 000/468 | Cost: 13942.4961\n",
      "Epoch: 010/050 | Batch 050/468 | Cost: 13660.9473\n",
      "Epoch: 010/050 | Batch 100/468 | Cost: 13481.7412\n",
      "Epoch: 010/050 | Batch 150/468 | Cost: 13702.4609\n",
      "Epoch: 010/050 | Batch 200/468 | Cost: 13722.1201\n",
      "Epoch: 010/050 | Batch 250/468 | Cost: 13537.0557\n",
      "Epoch: 010/050 | Batch 300/468 | Cost: 13314.1533\n",
      "Epoch: 010/050 | Batch 350/468 | Cost: 13305.7402\n",
      "Epoch: 010/050 | Batch 400/468 | Cost: 13054.1689\n",
      "Epoch: 010/050 | Batch 450/468 | Cost: 13054.4717\n",
      "Epoch: 011/050 | Batch 000/468 | Cost: 13788.7656\n",
      "Epoch: 011/050 | Batch 050/468 | Cost: 13925.3340\n",
      "Epoch: 011/050 | Batch 100/468 | Cost: 13168.6172\n",
      "Epoch: 011/050 | Batch 150/468 | Cost: 13955.8594\n",
      "Epoch: 011/050 | Batch 200/468 | Cost: 13674.8301\n",
      "Epoch: 011/050 | Batch 250/468 | Cost: 13678.7412\n",
      "Epoch: 011/050 | Batch 300/468 | Cost: 14204.8076\n",
      "Epoch: 011/050 | Batch 350/468 | Cost: 13616.2061\n",
      "Epoch: 011/050 | Batch 400/468 | Cost: 13484.1836\n",
      "Epoch: 011/050 | Batch 450/468 | Cost: 13901.6270\n",
      "Epoch: 012/050 | Batch 000/468 | Cost: 13045.0977\n",
      "Epoch: 012/050 | Batch 050/468 | Cost: 13072.7266\n",
      "Epoch: 012/050 | Batch 100/468 | Cost: 13175.8281\n",
      "Epoch: 012/050 | Batch 150/468 | Cost: 13296.7373\n",
      "Epoch: 012/050 | Batch 200/468 | Cost: 13429.0488\n",
      "Epoch: 012/050 | Batch 250/468 | Cost: 13693.1025\n",
      "Epoch: 012/050 | Batch 300/468 | Cost: 13098.0010\n",
      "Epoch: 012/050 | Batch 350/468 | Cost: 13431.5820\n",
      "Epoch: 012/050 | Batch 400/468 | Cost: 13656.7969\n",
      "Epoch: 012/050 | Batch 450/468 | Cost: 13382.0566\n",
      "Epoch: 013/050 | Batch 000/468 | Cost: 13799.8496\n",
      "Epoch: 013/050 | Batch 050/468 | Cost: 13800.6465\n",
      "Epoch: 013/050 | Batch 100/468 | Cost: 13441.7881\n",
      "Epoch: 013/050 | Batch 150/468 | Cost: 13784.5166\n",
      "Epoch: 013/050 | Batch 200/468 | Cost: 13315.3799\n",
      "Epoch: 013/050 | Batch 250/468 | Cost: 13385.3916\n",
      "Epoch: 013/050 | Batch 300/468 | Cost: 13839.4619\n",
      "Epoch: 013/050 | Batch 350/468 | Cost: 13292.1055\n",
      "Epoch: 013/050 | Batch 400/468 | Cost: 13343.7979\n",
      "Epoch: 013/050 | Batch 450/468 | Cost: 14235.7646\n",
      "Epoch: 014/050 | Batch 000/468 | Cost: 13438.9180\n",
      "Epoch: 014/050 | Batch 050/468 | Cost: 13078.0977\n",
      "Epoch: 014/050 | Batch 100/468 | Cost: 14003.8418\n",
      "Epoch: 014/050 | Batch 150/468 | Cost: 13924.0703\n",
      "Epoch: 014/050 | Batch 200/468 | Cost: 13715.2305\n",
      "Epoch: 014/050 | Batch 250/468 | Cost: 13638.3389\n",
      "Epoch: 014/050 | Batch 300/468 | Cost: 13512.7734\n",
      "Epoch: 014/050 | Batch 350/468 | Cost: 13747.3896\n",
      "Epoch: 014/050 | Batch 400/468 | Cost: 13522.3311\n",
      "Epoch: 014/050 | Batch 450/468 | Cost: 13695.3301\n",
      "Epoch: 015/050 | Batch 000/468 | Cost: 13422.3740\n",
      "Epoch: 015/050 | Batch 050/468 | Cost: 13288.2793\n",
      "Epoch: 015/050 | Batch 100/468 | Cost: 13436.0967\n",
      "Epoch: 015/050 | Batch 150/468 | Cost: 13153.0186\n",
      "Epoch: 015/050 | Batch 200/468 | Cost: 13301.6582\n",
      "Epoch: 015/050 | Batch 250/468 | Cost: 13776.9443\n",
      "Epoch: 015/050 | Batch 300/468 | Cost: 13650.6660\n",
      "Epoch: 015/050 | Batch 350/468 | Cost: 13384.9609\n",
      "Epoch: 015/050 | Batch 400/468 | Cost: 13381.6562\n",
      "Epoch: 015/050 | Batch 450/468 | Cost: 13585.0068\n",
      "Epoch: 016/050 | Batch 000/468 | Cost: 13171.7842\n",
      "Epoch: 016/050 | Batch 050/468 | Cost: 13400.5889\n",
      "Epoch: 016/050 | Batch 100/468 | Cost: 12988.0381\n",
      "Epoch: 016/050 | Batch 150/468 | Cost: 13147.8066\n",
      "Epoch: 016/050 | Batch 200/468 | Cost: 13245.1299\n",
      "Epoch: 016/050 | Batch 250/468 | Cost: 14335.0410\n",
      "Epoch: 016/050 | Batch 300/468 | Cost: 13811.4482\n",
      "Epoch: 016/050 | Batch 350/468 | Cost: 13863.8574\n",
      "Epoch: 016/050 | Batch 400/468 | Cost: 13232.7334\n",
      "Epoch: 016/050 | Batch 450/468 | Cost: 13325.2861\n",
      "Epoch: 017/050 | Batch 000/468 | Cost: 13298.5400\n",
      "Epoch: 017/050 | Batch 050/468 | Cost: 13565.3604\n",
      "Epoch: 017/050 | Batch 100/468 | Cost: 12945.6367\n",
      "Epoch: 017/050 | Batch 150/468 | Cost: 14110.0234\n",
      "Epoch: 017/050 | Batch 200/468 | Cost: 13328.3965\n",
      "Epoch: 017/050 | Batch 250/468 | Cost: 13966.9795\n",
      "Epoch: 017/050 | Batch 300/468 | Cost: 13357.1709\n",
      "Epoch: 017/050 | Batch 350/468 | Cost: 13306.5957\n",
      "Epoch: 017/050 | Batch 400/468 | Cost: 13869.9863\n",
      "Epoch: 017/050 | Batch 450/468 | Cost: 13267.2637\n",
      "Epoch: 018/050 | Batch 000/468 | Cost: 13019.0664\n",
      "Epoch: 018/050 | Batch 050/468 | Cost: 13502.3096\n",
      "Epoch: 018/050 | Batch 100/468 | Cost: 12789.1084\n",
      "Epoch: 018/050 | Batch 150/468 | Cost: 13654.0186\n",
      "Epoch: 018/050 | Batch 200/468 | Cost: 13252.5283\n",
      "Epoch: 018/050 | Batch 250/468 | Cost: 13470.0469\n",
      "Epoch: 018/050 | Batch 300/468 | Cost: 13250.1377\n",
      "Epoch: 018/050 | Batch 350/468 | Cost: 13481.7432\n",
      "Epoch: 018/050 | Batch 400/468 | Cost: 13310.3701\n",
      "Epoch: 018/050 | Batch 450/468 | Cost: 13406.0117\n",
      "Epoch: 019/050 | Batch 000/468 | Cost: 13351.6299\n",
      "Epoch: 019/050 | Batch 050/468 | Cost: 13358.8350\n",
      "Epoch: 019/050 | Batch 100/468 | Cost: 12937.6875\n",
      "Epoch: 019/050 | Batch 150/468 | Cost: 13421.7188\n",
      "Epoch: 019/050 | Batch 200/468 | Cost: 13403.1357\n",
      "Epoch: 019/050 | Batch 250/468 | Cost: 13814.0840\n",
      "Epoch: 019/050 | Batch 300/468 | Cost: 12169.6953\n",
      "Epoch: 019/050 | Batch 350/468 | Cost: 13136.0430\n",
      "Epoch: 019/050 | Batch 400/468 | Cost: 13107.0938\n",
      "Epoch: 019/050 | Batch 450/468 | Cost: 13394.4395\n",
      "Epoch: 020/050 | Batch 000/468 | Cost: 13443.2363\n",
      "Epoch: 020/050 | Batch 050/468 | Cost: 13536.2617\n",
      "Epoch: 020/050 | Batch 100/468 | Cost: 13229.0605\n",
      "Epoch: 020/050 | Batch 150/468 | Cost: 13564.6191\n",
      "Epoch: 020/050 | Batch 200/468 | Cost: 13702.7646\n",
      "Epoch: 020/050 | Batch 250/468 | Cost: 13443.2949\n",
      "Epoch: 020/050 | Batch 300/468 | Cost: 13410.3047\n",
      "Epoch: 020/050 | Batch 350/468 | Cost: 13179.8428\n",
      "Epoch: 020/050 | Batch 400/468 | Cost: 13593.3008\n",
      "Epoch: 020/050 | Batch 450/468 | Cost: 13660.1025\n",
      "Epoch: 021/050 | Batch 000/468 | Cost: 13567.2969\n",
      "Epoch: 021/050 | Batch 050/468 | Cost: 13163.6348\n",
      "Epoch: 021/050 | Batch 100/468 | Cost: 13278.3887\n",
      "Epoch: 021/050 | Batch 150/468 | Cost: 12863.3652\n",
      "Epoch: 021/050 | Batch 200/468 | Cost: 13052.3877\n",
      "Epoch: 021/050 | Batch 250/468 | Cost: 13218.4639\n",
      "Epoch: 021/050 | Batch 300/468 | Cost: 13397.7275\n",
      "Epoch: 021/050 | Batch 350/468 | Cost: 13844.2852\n",
      "Epoch: 021/050 | Batch 400/468 | Cost: 13483.2324\n",
      "Epoch: 021/050 | Batch 450/468 | Cost: 13589.3398\n",
      "Epoch: 022/050 | Batch 000/468 | Cost: 13655.2900\n",
      "Epoch: 022/050 | Batch 050/468 | Cost: 13410.6465\n",
      "Epoch: 022/050 | Batch 100/468 | Cost: 13333.2539\n",
      "Epoch: 022/050 | Batch 150/468 | Cost: 13539.0635\n",
      "Epoch: 022/050 | Batch 200/468 | Cost: 12949.8291\n",
      "Epoch: 022/050 | Batch 250/468 | Cost: 13154.8848\n",
      "Epoch: 022/050 | Batch 300/468 | Cost: 13188.1992\n",
      "Epoch: 022/050 | Batch 350/468 | Cost: 13634.7266\n",
      "Epoch: 022/050 | Batch 400/468 | Cost: 12866.5518\n",
      "Epoch: 022/050 | Batch 450/468 | Cost: 13241.0586\n",
      "Epoch: 023/050 | Batch 000/468 | Cost: 13102.9229\n",
      "Epoch: 023/050 | Batch 050/468 | Cost: 13432.0957\n",
      "Epoch: 023/050 | Batch 100/468 | Cost: 13057.7080\n",
      "Epoch: 023/050 | Batch 150/468 | Cost: 13331.5742\n",
      "Epoch: 023/050 | Batch 200/468 | Cost: 12866.7656\n",
      "Epoch: 023/050 | Batch 250/468 | Cost: 12981.3711\n",
      "Epoch: 023/050 | Batch 300/468 | Cost: 13777.9971\n",
      "Epoch: 023/050 | Batch 350/468 | Cost: 13483.8594\n",
      "Epoch: 023/050 | Batch 400/468 | Cost: 12822.0039\n",
      "Epoch: 023/050 | Batch 450/468 | Cost: 13255.9082\n",
      "Epoch: 024/050 | Batch 000/468 | Cost: 13452.7461\n",
      "Epoch: 024/050 | Batch 050/468 | Cost: 13571.0439\n",
      "Epoch: 024/050 | Batch 100/468 | Cost: 13530.6426\n",
      "Epoch: 024/050 | Batch 150/468 | Cost: 13398.5127\n",
      "Epoch: 024/050 | Batch 200/468 | Cost: 13239.0967\n",
      "Epoch: 024/050 | Batch 250/468 | Cost: 13644.0898\n",
      "Epoch: 024/050 | Batch 300/468 | Cost: 13701.7061\n",
      "Epoch: 024/050 | Batch 350/468 | Cost: 12828.1084\n",
      "Epoch: 024/050 | Batch 400/468 | Cost: 12980.6611\n",
      "Epoch: 024/050 | Batch 450/468 | Cost: 13028.9922\n",
      "Epoch: 025/050 | Batch 000/468 | Cost: 13569.9854\n",
      "Epoch: 025/050 | Batch 050/468 | Cost: 13183.2324\n",
      "Epoch: 025/050 | Batch 100/468 | Cost: 13465.9023\n",
      "Epoch: 025/050 | Batch 150/468 | Cost: 13187.2168\n",
      "Epoch: 025/050 | Batch 200/468 | Cost: 12863.3682\n",
      "Epoch: 025/050 | Batch 250/468 | Cost: 13063.7891\n",
      "Epoch: 025/050 | Batch 300/468 | Cost: 13394.6562\n",
      "Epoch: 025/050 | Batch 350/468 | Cost: 12745.8564\n",
      "Epoch: 025/050 | Batch 400/468 | Cost: 13272.4346\n",
      "Epoch: 025/050 | Batch 450/468 | Cost: 13436.5039\n",
      "Epoch: 026/050 | Batch 000/468 | Cost: 13171.5332\n",
      "Epoch: 026/050 | Batch 050/468 | Cost: 13431.2764\n",
      "Epoch: 026/050 | Batch 100/468 | Cost: 12845.9834\n",
      "Epoch: 026/050 | Batch 150/468 | Cost: 12826.1348\n",
      "Epoch: 026/050 | Batch 200/468 | Cost: 13587.0234\n",
      "Epoch: 026/050 | Batch 250/468 | Cost: 13029.3633\n",
      "Epoch: 026/050 | Batch 300/468 | Cost: 13143.5742\n",
      "Epoch: 026/050 | Batch 350/468 | Cost: 12934.1445\n",
      "Epoch: 026/050 | Batch 400/468 | Cost: 13640.5098\n",
      "Epoch: 026/050 | Batch 450/468 | Cost: 13285.1592\n",
      "Epoch: 027/050 | Batch 000/468 | Cost: 12820.5703\n",
      "Epoch: 027/050 | Batch 050/468 | Cost: 12726.4902\n",
      "Epoch: 027/050 | Batch 100/468 | Cost: 13254.5898\n",
      "Epoch: 027/050 | Batch 150/468 | Cost: 13106.8447\n",
      "Epoch: 027/050 | Batch 200/468 | Cost: 13217.8574\n",
      "Epoch: 027/050 | Batch 250/468 | Cost: 13151.5137\n",
      "Epoch: 027/050 | Batch 300/468 | Cost: 13267.2295\n",
      "Epoch: 027/050 | Batch 350/468 | Cost: 12914.3174\n",
      "Epoch: 027/050 | Batch 400/468 | Cost: 13329.8271\n",
      "Epoch: 027/050 | Batch 450/468 | Cost: 13371.7695\n",
      "Epoch: 028/050 | Batch 000/468 | Cost: 13475.7715\n",
      "Epoch: 028/050 | Batch 050/468 | Cost: 13261.4102\n",
      "Epoch: 028/050 | Batch 100/468 | Cost: 12976.0117\n",
      "Epoch: 028/050 | Batch 150/468 | Cost: 13084.5234\n",
      "Epoch: 028/050 | Batch 200/468 | Cost: 12551.8604\n",
      "Epoch: 028/050 | Batch 250/468 | Cost: 13072.8672\n",
      "Epoch: 028/050 | Batch 300/468 | Cost: 13131.9971\n",
      "Epoch: 028/050 | Batch 350/468 | Cost: 13576.4609\n",
      "Epoch: 028/050 | Batch 400/468 | Cost: 12872.9609\n",
      "Epoch: 028/050 | Batch 450/468 | Cost: 13292.8643\n",
      "Epoch: 029/050 | Batch 000/468 | Cost: 13244.3213\n",
      "Epoch: 029/050 | Batch 050/468 | Cost: 12967.8408\n",
      "Epoch: 029/050 | Batch 100/468 | Cost: 13416.4824\n",
      "Epoch: 029/050 | Batch 150/468 | Cost: 13135.0107\n",
      "Epoch: 029/050 | Batch 200/468 | Cost: 12858.8672\n",
      "Epoch: 029/050 | Batch 250/468 | Cost: 13258.4863\n",
      "Epoch: 029/050 | Batch 300/468 | Cost: 13188.1475\n",
      "Epoch: 029/050 | Batch 350/468 | Cost: 13063.3037\n",
      "Epoch: 029/050 | Batch 400/468 | Cost: 13110.2734\n",
      "Epoch: 029/050 | Batch 450/468 | Cost: 13172.0078\n",
      "Epoch: 030/050 | Batch 000/468 | Cost: 13187.1777\n",
      "Epoch: 030/050 | Batch 050/468 | Cost: 13412.8857\n",
      "Epoch: 030/050 | Batch 100/468 | Cost: 12928.9199\n",
      "Epoch: 030/050 | Batch 150/468 | Cost: 13273.9600\n",
      "Epoch: 030/050 | Batch 200/468 | Cost: 12810.7900\n",
      "Epoch: 030/050 | Batch 250/468 | Cost: 12841.0869\n",
      "Epoch: 030/050 | Batch 300/468 | Cost: 12690.1416\n",
      "Epoch: 030/050 | Batch 350/468 | Cost: 13426.8359\n",
      "Epoch: 030/050 | Batch 400/468 | Cost: 13770.0938\n",
      "Epoch: 030/050 | Batch 450/468 | Cost: 13093.9365\n",
      "Epoch: 031/050 | Batch 000/468 | Cost: 13015.1934\n",
      "Epoch: 031/050 | Batch 050/468 | Cost: 13321.2881\n",
      "Epoch: 031/050 | Batch 100/468 | Cost: 13313.8027\n",
      "Epoch: 031/050 | Batch 150/468 | Cost: 13195.7588\n",
      "Epoch: 031/050 | Batch 200/468 | Cost: 13091.0430\n",
      "Epoch: 031/050 | Batch 250/468 | Cost: 13382.8213\n",
      "Epoch: 031/050 | Batch 300/468 | Cost: 13057.2402\n",
      "Epoch: 031/050 | Batch 350/468 | Cost: 13435.7422\n",
      "Epoch: 031/050 | Batch 400/468 | Cost: 13546.5664\n",
      "Epoch: 031/050 | Batch 450/468 | Cost: 13056.8330\n",
      "Epoch: 032/050 | Batch 000/468 | Cost: 13251.3428\n",
      "Epoch: 032/050 | Batch 050/468 | Cost: 12833.8867\n",
      "Epoch: 032/050 | Batch 100/468 | Cost: 13414.9824\n",
      "Epoch: 032/050 | Batch 150/468 | Cost: 12835.0020\n",
      "Epoch: 032/050 | Batch 200/468 | Cost: 13086.7520\n",
      "Epoch: 032/050 | Batch 250/468 | Cost: 12916.9590\n",
      "Epoch: 032/050 | Batch 300/468 | Cost: 13226.1562\n",
      "Epoch: 032/050 | Batch 350/468 | Cost: 13029.9932\n",
      "Epoch: 032/050 | Batch 400/468 | Cost: 13138.2207\n",
      "Epoch: 032/050 | Batch 450/468 | Cost: 13216.4629\n",
      "Epoch: 033/050 | Batch 000/468 | Cost: 12960.5742\n",
      "Epoch: 033/050 | Batch 050/468 | Cost: 13238.2363\n",
      "Epoch: 033/050 | Batch 100/468 | Cost: 13200.7139\n",
      "Epoch: 033/050 | Batch 150/468 | Cost: 13087.1465\n",
      "Epoch: 033/050 | Batch 200/468 | Cost: 13002.6738\n",
      "Epoch: 033/050 | Batch 250/468 | Cost: 12968.4268\n",
      "Epoch: 033/050 | Batch 300/468 | Cost: 12889.2119\n",
      "Epoch: 033/050 | Batch 350/468 | Cost: 13553.4482\n",
      "Epoch: 033/050 | Batch 400/468 | Cost: 13058.3066\n",
      "Epoch: 033/050 | Batch 450/468 | Cost: 13284.2021\n",
      "Epoch: 034/050 | Batch 000/468 | Cost: 12995.0996\n",
      "Epoch: 034/050 | Batch 050/468 | Cost: 12961.0391\n",
      "Epoch: 034/050 | Batch 100/468 | Cost: 13311.6670\n",
      "Epoch: 034/050 | Batch 150/468 | Cost: 13335.8955\n",
      "Epoch: 034/050 | Batch 200/468 | Cost: 13327.5645\n",
      "Epoch: 034/050 | Batch 250/468 | Cost: 13135.8848\n",
      "Epoch: 034/050 | Batch 300/468 | Cost: 13240.4717\n",
      "Epoch: 034/050 | Batch 350/468 | Cost: 13266.4707\n",
      "Epoch: 034/050 | Batch 400/468 | Cost: 12813.7891\n",
      "Epoch: 034/050 | Batch 450/468 | Cost: 12835.9473\n",
      "Epoch: 035/050 | Batch 000/468 | Cost: 12722.5586\n",
      "Epoch: 035/050 | Batch 050/468 | Cost: 12520.9668\n",
      "Epoch: 035/050 | Batch 100/468 | Cost: 12332.8936\n",
      "Epoch: 035/050 | Batch 150/468 | Cost: 12861.8428\n",
      "Epoch: 035/050 | Batch 200/468 | Cost: 13028.7617\n",
      "Epoch: 035/050 | Batch 250/468 | Cost: 13551.6221\n",
      "Epoch: 035/050 | Batch 300/468 | Cost: 13464.6729\n",
      "Epoch: 035/050 | Batch 350/468 | Cost: 12824.0498\n",
      "Epoch: 035/050 | Batch 400/468 | Cost: 12652.2471\n",
      "Epoch: 035/050 | Batch 450/468 | Cost: 12754.0645\n",
      "Epoch: 036/050 | Batch 000/468 | Cost: 13105.0088\n",
      "Epoch: 036/050 | Batch 050/468 | Cost: 13051.4092\n",
      "Epoch: 036/050 | Batch 100/468 | Cost: 12660.4004\n",
      "Epoch: 036/050 | Batch 150/468 | Cost: 13104.5586\n",
      "Epoch: 036/050 | Batch 200/468 | Cost: 13464.3145\n",
      "Epoch: 036/050 | Batch 250/468 | Cost: 12631.1104\n",
      "Epoch: 036/050 | Batch 300/468 | Cost: 13243.8916\n",
      "Epoch: 036/050 | Batch 350/468 | Cost: 13341.4082\n",
      "Epoch: 036/050 | Batch 400/468 | Cost: 12988.3027\n",
      "Epoch: 036/050 | Batch 450/468 | Cost: 12911.6270\n",
      "Epoch: 037/050 | Batch 000/468 | Cost: 12612.4590\n",
      "Epoch: 037/050 | Batch 050/468 | Cost: 13073.4570\n",
      "Epoch: 037/050 | Batch 100/468 | Cost: 12714.2021\n",
      "Epoch: 037/050 | Batch 150/468 | Cost: 12626.4170\n",
      "Epoch: 037/050 | Batch 200/468 | Cost: 12565.5957\n",
      "Epoch: 037/050 | Batch 250/468 | Cost: 12451.6201\n",
      "Epoch: 037/050 | Batch 300/468 | Cost: 13343.7168\n",
      "Epoch: 037/050 | Batch 350/468 | Cost: 13048.6660\n",
      "Epoch: 037/050 | Batch 400/468 | Cost: 13252.7891\n",
      "Epoch: 037/050 | Batch 450/468 | Cost: 13200.8105\n",
      "Epoch: 038/050 | Batch 000/468 | Cost: 12493.7861\n",
      "Epoch: 038/050 | Batch 050/468 | Cost: 13581.4609\n",
      "Epoch: 038/050 | Batch 100/468 | Cost: 13024.2832\n",
      "Epoch: 038/050 | Batch 150/468 | Cost: 13081.6963\n",
      "Epoch: 038/050 | Batch 200/468 | Cost: 13090.4648\n",
      "Epoch: 038/050 | Batch 250/468 | Cost: 13193.0537\n",
      "Epoch: 038/050 | Batch 300/468 | Cost: 12620.1426\n",
      "Epoch: 038/050 | Batch 350/468 | Cost: 12704.0957\n",
      "Epoch: 038/050 | Batch 400/468 | Cost: 12816.0664\n",
      "Epoch: 038/050 | Batch 450/468 | Cost: 12753.6172\n",
      "Epoch: 039/050 | Batch 000/468 | Cost: 12908.4814\n",
      "Epoch: 039/050 | Batch 050/468 | Cost: 13033.1787\n",
      "Epoch: 039/050 | Batch 100/468 | Cost: 12982.5264\n",
      "Epoch: 039/050 | Batch 150/468 | Cost: 12915.5049\n",
      "Epoch: 039/050 | Batch 200/468 | Cost: 12689.0811\n",
      "Epoch: 039/050 | Batch 250/468 | Cost: 12952.7490\n",
      "Epoch: 039/050 | Batch 300/468 | Cost: 13063.2637\n",
      "Epoch: 039/050 | Batch 350/468 | Cost: 12938.9121\n",
      "Epoch: 039/050 | Batch 400/468 | Cost: 12863.3438\n",
      "Epoch: 039/050 | Batch 450/468 | Cost: 13354.1904\n",
      "Epoch: 040/050 | Batch 000/468 | Cost: 12663.1055\n",
      "Epoch: 040/050 | Batch 050/468 | Cost: 13490.5117\n",
      "Epoch: 040/050 | Batch 100/468 | Cost: 12838.5986\n",
      "Epoch: 040/050 | Batch 150/468 | Cost: 12668.1904\n",
      "Epoch: 040/050 | Batch 200/468 | Cost: 13257.1582\n",
      "Epoch: 040/050 | Batch 250/468 | Cost: 13047.5713\n",
      "Epoch: 040/050 | Batch 300/468 | Cost: 12936.5078\n",
      "Epoch: 040/050 | Batch 350/468 | Cost: 12614.2969\n",
      "Epoch: 040/050 | Batch 400/468 | Cost: 12967.1318\n",
      "Epoch: 040/050 | Batch 450/468 | Cost: 12697.6650\n",
      "Epoch: 041/050 | Batch 000/468 | Cost: 12755.3350\n",
      "Epoch: 041/050 | Batch 050/468 | Cost: 12409.8613\n",
      "Epoch: 041/050 | Batch 100/468 | Cost: 12690.7285\n",
      "Epoch: 041/050 | Batch 150/468 | Cost: 13416.9531\n",
      "Epoch: 041/050 | Batch 200/468 | Cost: 12762.5273\n",
      "Epoch: 041/050 | Batch 250/468 | Cost: 12854.1523\n",
      "Epoch: 041/050 | Batch 300/468 | Cost: 13051.2227\n",
      "Epoch: 041/050 | Batch 350/468 | Cost: 13387.7070\n",
      "Epoch: 041/050 | Batch 400/468 | Cost: 13369.6133\n",
      "Epoch: 041/050 | Batch 450/468 | Cost: 12887.8125\n",
      "Epoch: 042/050 | Batch 000/468 | Cost: 12787.5332\n",
      "Epoch: 042/050 | Batch 050/468 | Cost: 13315.7539\n",
      "Epoch: 042/050 | Batch 100/468 | Cost: 13111.9922\n",
      "Epoch: 042/050 | Batch 150/468 | Cost: 12960.4482\n",
      "Epoch: 042/050 | Batch 200/468 | Cost: 13392.6289\n",
      "Epoch: 042/050 | Batch 250/468 | Cost: 13263.4707\n",
      "Epoch: 042/050 | Batch 300/468 | Cost: 12927.4795\n",
      "Epoch: 042/050 | Batch 350/468 | Cost: 12863.0527\n",
      "Epoch: 042/050 | Batch 400/468 | Cost: 13257.0625\n",
      "Epoch: 042/050 | Batch 450/468 | Cost: 12651.1699\n",
      "Epoch: 043/050 | Batch 000/468 | Cost: 13095.9727\n",
      "Epoch: 043/050 | Batch 050/468 | Cost: 12990.5410\n",
      "Epoch: 043/050 | Batch 100/468 | Cost: 12674.2900\n",
      "Epoch: 043/050 | Batch 150/468 | Cost: 12930.8926\n",
      "Epoch: 043/050 | Batch 200/468 | Cost: 13279.1719\n",
      "Epoch: 043/050 | Batch 250/468 | Cost: 12672.7510\n",
      "Epoch: 043/050 | Batch 300/468 | Cost: 12884.5352\n",
      "Epoch: 043/050 | Batch 350/468 | Cost: 13228.5811\n",
      "Epoch: 043/050 | Batch 400/468 | Cost: 12641.7354\n",
      "Epoch: 043/050 | Batch 450/468 | Cost: 13235.4912\n",
      "Epoch: 044/050 | Batch 000/468 | Cost: 13523.4971\n",
      "Epoch: 044/050 | Batch 050/468 | Cost: 13010.3857\n",
      "Epoch: 044/050 | Batch 100/468 | Cost: 12760.5059\n",
      "Epoch: 044/050 | Batch 150/468 | Cost: 13469.3301\n",
      "Epoch: 044/050 | Batch 200/468 | Cost: 13257.1982\n",
      "Epoch: 044/050 | Batch 250/468 | Cost: 13436.6309\n",
      "Epoch: 044/050 | Batch 300/468 | Cost: 13071.4160\n",
      "Epoch: 044/050 | Batch 350/468 | Cost: 13571.0156\n",
      "Epoch: 044/050 | Batch 400/468 | Cost: 12692.9141\n",
      "Epoch: 044/050 | Batch 450/468 | Cost: 12798.2588\n",
      "Epoch: 045/050 | Batch 000/468 | Cost: 12711.7139\n",
      "Epoch: 045/050 | Batch 050/468 | Cost: 12736.5332\n",
      "Epoch: 045/050 | Batch 100/468 | Cost: 13062.4717\n",
      "Epoch: 045/050 | Batch 150/468 | Cost: 13041.7373\n",
      "Epoch: 045/050 | Batch 200/468 | Cost: 12900.5742\n",
      "Epoch: 045/050 | Batch 250/468 | Cost: 13381.3105\n",
      "Epoch: 045/050 | Batch 300/468 | Cost: 12926.7676\n",
      "Epoch: 045/050 | Batch 350/468 | Cost: 12839.4004\n",
      "Epoch: 045/050 | Batch 400/468 | Cost: 13339.0742\n",
      "Epoch: 045/050 | Batch 450/468 | Cost: 12865.7100\n",
      "Epoch: 046/050 | Batch 000/468 | Cost: 12741.0625\n",
      "Epoch: 046/050 | Batch 050/468 | Cost: 13013.3701\n",
      "Epoch: 046/050 | Batch 100/468 | Cost: 13016.3311\n",
      "Epoch: 046/050 | Batch 150/468 | Cost: 12964.6523\n",
      "Epoch: 046/050 | Batch 200/468 | Cost: 12582.7451\n",
      "Epoch: 046/050 | Batch 250/468 | Cost: 13312.0176\n",
      "Epoch: 046/050 | Batch 300/468 | Cost: 12783.7822\n",
      "Epoch: 046/050 | Batch 350/468 | Cost: 12342.4717\n",
      "Epoch: 046/050 | Batch 400/468 | Cost: 13206.8555\n",
      "Epoch: 046/050 | Batch 450/468 | Cost: 12555.6602\n",
      "Epoch: 047/050 | Batch 000/468 | Cost: 13027.8877\n",
      "Epoch: 047/050 | Batch 050/468 | Cost: 12920.7617\n",
      "Epoch: 047/050 | Batch 100/468 | Cost: 12964.8047\n",
      "Epoch: 047/050 | Batch 150/468 | Cost: 12596.6953\n",
      "Epoch: 047/050 | Batch 200/468 | Cost: 12951.7031\n",
      "Epoch: 047/050 | Batch 250/468 | Cost: 12869.5869\n",
      "Epoch: 047/050 | Batch 300/468 | Cost: 13008.6572\n",
      "Epoch: 047/050 | Batch 350/468 | Cost: 13464.5293\n",
      "Epoch: 047/050 | Batch 400/468 | Cost: 13363.3516\n",
      "Epoch: 047/050 | Batch 450/468 | Cost: 13042.6367\n",
      "Epoch: 048/050 | Batch 000/468 | Cost: 12616.9492\n",
      "Epoch: 048/050 | Batch 050/468 | Cost: 12994.8086\n",
      "Epoch: 048/050 | Batch 100/468 | Cost: 12974.7305\n",
      "Epoch: 048/050 | Batch 150/468 | Cost: 13471.7393\n",
      "Epoch: 048/050 | Batch 200/468 | Cost: 12665.0703\n",
      "Epoch: 048/050 | Batch 250/468 | Cost: 13208.9834\n",
      "Epoch: 048/050 | Batch 300/468 | Cost: 13169.7822\n",
      "Epoch: 048/050 | Batch 350/468 | Cost: 13393.3975\n",
      "Epoch: 048/050 | Batch 400/468 | Cost: 12845.1953\n",
      "Epoch: 048/050 | Batch 450/468 | Cost: 12967.8994\n",
      "Epoch: 049/050 | Batch 000/468 | Cost: 12864.6211\n",
      "Epoch: 049/050 | Batch 050/468 | Cost: 12801.3262\n",
      "Epoch: 049/050 | Batch 100/468 | Cost: 13168.2217\n",
      "Epoch: 049/050 | Batch 150/468 | Cost: 12824.5127\n",
      "Epoch: 049/050 | Batch 200/468 | Cost: 12847.0840\n",
      "Epoch: 049/050 | Batch 250/468 | Cost: 13238.8682\n",
      "Epoch: 049/050 | Batch 300/468 | Cost: 12902.1211\n",
      "Epoch: 049/050 | Batch 350/468 | Cost: 13068.8555\n",
      "Epoch: 049/050 | Batch 400/468 | Cost: 12974.8252\n",
      "Epoch: 049/050 | Batch 450/468 | Cost: 12781.5693\n",
      "Epoch: 050/050 | Batch 000/468 | Cost: 12833.4062\n",
      "Epoch: 050/050 | Batch 050/468 | Cost: 12780.1631\n",
      "Epoch: 050/050 | Batch 100/468 | Cost: 12714.4219\n",
      "Epoch: 050/050 | Batch 150/468 | Cost: 13303.0605\n",
      "Epoch: 050/050 | Batch 200/468 | Cost: 13703.2412\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # don't need labels, only the images (features)\n",
    "        features = features.view(-1, 28*28).to(device)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        z_mean, z_log_var, encoded, decoded = model(features)\n",
    "\n",
    "        # cost = reconstruction loss + Kullback-Leibler divergence\n",
    "        kl_divergence = (0.5 * (z_mean**2 + \n",
    "                                torch.exp(z_log_var) - z_log_var - 1)).sum()\n",
    "        pixelwise_bce = F.binary_cross_entropy(decoded, features, reduction='sum')\n",
    "        cost = kl_divergence + pixelwise_bce\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_dataset)//batch_size, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##########################\n",
    "### VISUALIZATION\n",
    "##########################\n",
    "\n",
    "n_images = 15\n",
    "image_width = 28\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_images, \n",
    "                         sharex=True, sharey=True, figsize=(20, 2.5))\n",
    "orig_images = features[:n_images]\n",
    "decoded_images = decoded[:n_images]\n",
    "\n",
    "for i in range(n_images):\n",
    "    for ax, img in zip(axes, [orig_images, decoded_images]):\n",
    "        ax[i].imshow(img[i].detach().reshape((image_width, image_width)), cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    ##########################\n",
    "    ### RANDOM SAMPLE\n",
    "    ##########################    \n",
    "    \n",
    "    n_images = 10\n",
    "    rand_features = torch.randn(n_images, num_latent).to(device)\n",
    "    new_images = model.decoder(rand_features)\n",
    "\n",
    "    ##########################\n",
    "    ### VISUALIZATION\n",
    "    ##########################\n",
    "\n",
    "    image_width = 28\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_images, figsize=(10, 2.5), sharey=True)\n",
    "    decoded_images = new_images[:n_images]\n",
    "\n",
    "    for ax, img in zip(axes, decoded_images):\n",
    "        ax.imshow(img.detach().reshape((image_width, image_width)), cmap='binary')\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
