{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 6.5.0\n",
      "\n",
      "torch 0.4.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Using PyTorch Dataset Loading Utilities for Custom Datasets (Images from Quickdraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example for how to load an image dataset, stored as individual PNG files, using PyTorch's data loading utilities. For a more in-depth discussion, please see the official\n",
    "\n",
    "- [Data Loading and Processing Tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "- [torch.utils.data](http://pytorch.org/docs/master/data.html) API documentation\n",
    "\n",
    "In this example, we are using the Quickdraw dataset consisting of handdrawn objects, which is available at https://quickdraw.withgoogle.com. \n",
    "\n",
    "To execute the following examples, you need to download the \".npy\" (bitmap files in NumPy). You don't need to download all of the 345 categories but only a subset you are interested in. The groups/subsets can be individually downloaded from https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap\n",
    "\n",
    "Unfortunately, the Google cloud storage currently does not support selecting and downloading multiple groups at once. Thus, in order to download all groups most coneniently, we need to use their `gsutil` (https://cloud.google.com/storage/docs/gsutil_install) tool. If you want to install that, you can then use \n",
    "\n",
    "    mkdir quickdraw-npy\n",
    "    gsutil -m cp gs://quickdraw_dataset/full/numpy_bitmap/*.npy quickdraw-npy\n",
    "\n",
    "Note that if you download the whole dataset, this will take up 37 Gb of storage space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset to a local directory, `quickdraw-npy`, the next step is to select certain groups we are interested in analyzing. Let's say we are interested in the following groups defined in the `label_dict` in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "         \"lollipop\": 0,\n",
    "         \"binoculars\": 1,\n",
    "         \"mouse\": 2,\n",
    "         \"basket\": 3,\n",
    "         \"penguin\": 4,\n",
    "         \"washing machine\": 5,\n",
    "         \"canoe\": 6,\n",
    "         \"eyeglasses\": 7,\n",
    "         \"beach\": 8,\n",
    "         \"screwdriver\": 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary values shall represent class labels that we could use for a classification task, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to PNG files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to convert the groups we are interested in (specified in the dictionary above) to individual PNG files using a helper function (note that this might take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import quickdraw_npy_to_imagefile\n",
    "\n",
    "    \n",
    "quickdraw_npy_to_imagefile(inpath='quickdraw-npy',\n",
    "                           outpath='quickdraw-png_set1',\n",
    "                           subset=label_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing into train/valid/test subsets and creating a label files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's create a CSV file mapping file names to class labels. First, let's collect the files and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num paths: 1515745\n",
      "Num labels: 1515745\n"
     ]
    }
   ],
   "source": [
    "paths, labels = [], []\n",
    "\n",
    "main_dir = 'quickdraw-png_set1/'\n",
    "\n",
    "for d in os.listdir(main_dir):\n",
    "    subdir = os.path.join(main_dir, d)\n",
    "    if not os.path.isdir(subdir):\n",
    "        continue\n",
    "    for f in os.listdir(subdir):\n",
    "        path = os.path.join(d, f)\n",
    "        paths.append(path)\n",
    "        labels.append(label_dict[d])\n",
    "        \n",
    "print('Num paths:', len(paths))\n",
    "print('Num labels:', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shuffle the dataset and assign 70% of the dataset for training, 10% for validation, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "\n",
    "\n",
    "paths2, labels2 = shuffle_arrays_unison(arrays=[np.array(paths), np.array(labels)], random_seed=3)\n",
    "\n",
    "\n",
    "cut1 = int(len(paths)*0.7)\n",
    "cut2 = int(len(paths)*0.8)\n",
    "\n",
    "paths_train, labels_train = paths2[:cut1], labels2[:cut1]\n",
    "paths_valid, labels_valid = paths2[cut1:cut2], labels2[cut1:cut2]\n",
    "paths_test, labels_test = paths2[cut2:], labels2[cut2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us create a CSV file that maps the file paths to the class labels (here only shown for the training set for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Path</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>binoculars/binoculars_022693.png</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screwdriver/screwdriver_090188.png</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beach/beach_105009.png</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penguin/penguin_179153.png</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basket/basket_074384.png</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Label\n",
       "Path                                     \n",
       "binoculars/binoculars_022693.png        1\n",
       "screwdriver/screwdriver_090188.png      9\n",
       "beach/beach_105009.png                  8\n",
       "penguin/penguin_179153.png              4\n",
       "basket/basket_074384.png                3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'Path': paths_train,\n",
    "     'Label': labels_train,\n",
    "    })\n",
    "\n",
    "df = df.set_index('Path')\n",
    "df.to_csv('quickdraw_png_set1_train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's open one of the images to make sure they look ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_dir = 'quickdraw-png_set1/'\n",
    "\n",
    "img = Image.open(os.path.join(main_dir, df.index[99]))\n",
    "img = np.asarray(img, dtype=np.uint8)\n",
    "print(img.shape)\n",
    "plt.imshow(np.array(img), cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Custom Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a custom `Dataset` for reading the images. The `__getitem__` method will\n",
    "\n",
    "1. read a single image from disk based on an `index` (more on batching later)\n",
    "2. perform a custom image transformation (if a `transform` argument is provided in the `__init__` construtor)\n",
    "3. return a single image and it's corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickdrawDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading Quickdraw images\"\"\"\n",
    "\n",
    "    def __init__(self, txt_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(txt_path, sep=\",\", index_col=0)\n",
    "        self.img_dir = img_dir\n",
    "        self.txt_path = txt_path\n",
    "        self.img_names = df.index.values\n",
    "        self.y = df['Label'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our custom Dataset class, let us add some custom transformations via the `transforms` utilities from `torchvision`, we\n",
    "\n",
    "1. normalize the images (here: dividing by 255)\n",
    "2. converting the image arrays into PyTorch tensors\n",
    "\n",
    "Then, we initialize a Dataset instance for the training images using the 'quickdraw_png_set1_train.csv' label file (we omit the test set, but the same concepts apply).\n",
    "\n",
    "Finally, we initialize a `DataLoader` that allows us to read from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that transforms.ToTensor()\n",
    "# already divides pixels by 255. internally\n",
    "\n",
    "custom_transform = transforms.Compose([#transforms.Lambda(lambda x: x/255.),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "train_dataset = QuickdrawDataset(txt_path='quickdraw_png_set1_train.csv',\n",
    "                                 img_dir='quickdraw-png_set1/',\n",
    "                                 transform=custom_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, now we can iterate over an epoch using the train_loader as an iterator and use the features and labels from the training dataset for model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Through the Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure that the batches are being loaded correctly, let's print out the dimensions of the last batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each batch consists of 128 images, just as specified. However, one thing to keep in mind though is that\n",
    "PyTorch uses a different image layout (which is more efficient when working with CUDA); here, the image axes are \"num_images x channels x height x width\" (NCHW) instead of \"num_images height x width x channels\" (NHWC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visually check that the images that coming of the data loader are intact, let's swap the axes to NHWC and convert an image from a Torch Tensor to a NumPy array so that we can visualize the image via `imshow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_image = x[0].permute(1, 2, 0)\n",
    "one_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAADrRJREFUeJzt3X+sVPWZx/HPIxRNaENArkoA99aqG0Vc0BE1blY3itoNCRKDgZgGI0oTqy4JiYuaWP/RCNm2q9GY4EJK/dWSUIUo2a0xVcSYymgUbFm3Ru+WX4GLNkGI8kOe/eMemive8z3DzJk5g8/7lZA7c5753vM4+OHMzHfO+Zq7C0A8J1XdAIBqEH4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0EN7+TOxo4d6729vZ3cJRBKX1+f9uzZY408tqXwm9n1kh6VNEzSf7r7I6nH9/b2ql6vt7JLAAm1Wq3hxzb9st/Mhkl6QtIPJZ0vaa6Znd/s7wPQWa28558m6SN3/9jdD0r6taSZ5bQFoN1aCf94SVsH3d+WbfsaM1tgZnUzq/f397ewOwBlaiX8Q32o8I3zg919mbvX3L3W09PTwu4AlKmV8G+TNHHQ/QmSdrTWDoBOaSX8GyWdY2bfN7MRkuZIWltOWwDarempPnc/bGZ3SvpvDUz1rXD3P5bWGYC2amme393XSVpXUi8AOoiv9wJBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUS6v0mlmfpM8lfSXpsLvXymgKQPu1FP7MP7v7nhJ+D4AO4mU/EFSr4XdJvzOzd8xsQRkNAeiMVl/2X+HuO8zsNEmvmNn/uPv6wQ/I/lFYIElnnnlmi7sDUJaWjvzuviP7uVvSC5KmDfGYZe5ec/daT09PK7sDUKKmw29mI83se0dvS7pW0gdlNQagvVp52X+6pBfM7Ojvec7d/6uUrgC0XdPhd/ePJf1Dib0A6CCm+oCgCD8QFOEHgiL8QFCEHwiK8ANBlXFWHyp26NCh3Nq+ffuSY909Wd+7d2+yXvSV7ZNO4vjSrfibAYIi/EBQhB8IivADQRF+ICjCDwRF+IGgmOfvgO3btyfra9euTdZffvnlZP21117Lre3fvz85tlVF8/zz589vqiZJ48ePb6onNIYjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZUXnc5epVqt5vV7v2P46ZenSpcn6Aw88kKwfOHAgWZ80aVKyft111+XWzj333OTYbN2FXMOHp78KsmbNmmT9pZdeStZTpk37xgJQX3P22Wcn62eddVZubfLkycmxs2bNStaHDRuWrFelVqupXq+n/1IzHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKjC8/nNbIWkGZJ2u/sF2bYxkn4jqVdSn6Sb3P2v7WuzuxWdr3/77bcn64sWLUrWe3t7j7eljrn11luT9dRzs2LFiuTYt956K1nfuHFjsr5q1arc2sGDB5Njp0yZkqw/9NBDyfo111yTrI8YMSJZ74RGjvy/lHT9MdsWS3rV3c+R9Gp2H8AJpDD87r5e0mfHbJ4paWV2e6WkG0ruC0CbNfue/3R33ylJ2c/TymsJQCe0/QM/M1tgZnUzq/f397d7dwAa1Gz4d5nZOEnKfu7Oe6C7L3P3mrvXenp6mtwdgLI1G/61kuZlt+dJSp/aBaDrFIbfzJ6X9JakvzezbWY2X9Ijkqab2Z8lTc/uAziBhDmff+vWrcn6hx9+mKyPGjUqt9bqGvSnnnpqst7N8/zd7Msvv8ytTZ8+PTl2w4YNLe37wgsvTNbff//9ln5/Hs7nB1CI8ANBEX4gKMIPBEX4gaAIPxDUCbVE96FDh3JrS5YsSY4tOgUzNS1UtQkTJiTrV199dW6taAntt99+O1kfM2ZMsv7MM88k661Og7Zi8eL8k03ffPPN5Niiy4YXPW+bNm1K1pcvX55bK1q6vCwc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqK6a5z98+HCynprPfuONN5Jjb7nllmS9aG61le8BFP13bdu2LVl//fXXk/V169bl1tp96bSiy2vfcccdubWiy34XnWb94osvJutPPvlkbu3mm29Ojn366aeT9c2bNyfr9957b7Keupx70Xczik5HbhRHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iqqvm+VevXp2sp+byn3vuueTYuXPnNtVTN7jtttuS9UsvvTS3VnRp9qLn7cCBA8n6448/nqzfc889ubXU+faSdOTIkWT95JNPTtZTy3B/+umnybFFJk+enKw/8cQTyXrqcux9fX1NdHT8OPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCF8/xmtkLSDEm73f2CbNuDkm6XdPRk8fvcPf+k8gbt2LEjWR8xYkRubfbs2a3uvmt98cUXyXrqGvJLly5Njm313PAZM2Yk6+vXr8+tXXnllcmx5513XrL+6KOPJuvXXnttbu2uu+5Kji1StOT7/fff3/Tvvuiii5oeezwaOfL/UtL1Q2z/hbtPyf60HHwAnVUYfndfL+mzDvQCoINaec9/p5ltMrMVZja6tI4AdESz4X9S0g8kTZG0U9LP8h5oZgvMrG5m9XZfTw5A45oKv7vvcvev3P2IpKck5a5q6O7L3L3m7rWenp5m+wRQsqbCb2bjBt2dJemDctoB0CmNTPU9L+kqSWPNbJukn0q6ysymSHJJfZJ+3MYeAbRBYfjdfagT4fMXF29B0XXcL7vsstza8OFddWmCUu3Zs6fpsWeccUaJnRy/qVOn5tbGjBmTHHvJJZck66NGjWqqJ6l4nYZTTjklWd++fXuyftJJ6RfVDz/8cG7t4osvTo4tC9/wA4Ii/EBQhB8IivADQRF+ICjCDwTVVfNjRVM3l19+eYc6ObGYWW6taJnrVhWdbnzjjTfm1vbt25ccu3DhwmQ9NY0oSY899lhurWhJ9yJz5sxJ1u++++5kveopWIkjPxAW4QeCIvxAUIQfCIrwA0ERfiAowg8E1VXz/BjaxIkTk/XUEt5LlixJji06Xbjo8tlFS1F/8sknubVnn302ObZoHr9I6vLcrV66+9uAIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU8/7dAaqnqkSNHJsc+9dRTyfr+/fuT9UmTJiXrqfPmU5diR/tx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMzd0w8wmyjpV5LOkHRE0jJ3f9TMxkj6jaReSX2SbnL3v6Z+V61W83q9XkLb6JT+/v5kffTo0cn6t3np9G5Uq9VUr9fzF3IYpJEj/2FJi9z9PEmXSfqJmZ0vabGkV939HEmvZvcBnCAKw+/uO9393ez255K2SBovaaakldnDVkq6oV1NAijfcb3nN7NeSVMl/UHS6e6+Uxr4B0LSaWU3B6B9Gg6/mX1X0mpJC91973GMW2BmdTOrF71/BNA5DYXfzL6jgeA/6+6/zTbvMrNxWX2cpN1DjXX3Ze5ec/daT09PGT0DKEFh+G1gCdjlkra4+88HldZKmpfdnidpTfntAWiXRuZhrpD0I0mbzey9bNt9kh6RtMrM5kv6i6TZ7WkRVeLV2rdXYfjdfYOkvHnDq8ttB0Cn8A0/ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCF4TeziWb2ezPbYmZ/NLN/zbY/aGbbzey97M+/tL9dAGUZ3sBjDkta5O7vmtn3JL1jZq9ktV+4+7+3rz0A7VIYfnffKWlndvtzM9siaXy7GwPQXsf1nt/MeiVNlfSHbNOdZrbJzFaY2eicMQvMrG5m9f7+/paaBVCehsNvZt+VtFrSQnffK+lJST+QNEUDrwx+NtQ4d1/m7jV3r/X09JTQMoAyNBR+M/uOBoL/rLv/VpLcfZe7f+XuRyQ9JWla+9oEULZGPu03ScslbXH3nw/aPm7Qw2ZJ+qD89gC0SyOf9l8h6UeSNpvZe9m2+yTNNbMpklxSn6Qft6VDAG3RyKf9GyTZEKV15bcDoFP4hh8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAoc/fO7cysX9L/Ddo0VtKejjVwfLq1t27tS6K3ZpXZ29+5e0PXy+to+L+xc7O6u9cqayChW3vr1r4kemtWVb3xsh8IivADQVUd/mUV7z+lW3vr1r4kemtWJb1V+p4fQHWqPvIDqEgl4Tez683sQzP7yMwWV9FDHjPrM7PN2crD9Yp7WWFmu83sg0HbxpjZK2b25+znkMukVdRbV6zcnFhZutLnrttWvO74y34zGybpfyVNl7RN0kZJc939Tx1tJIeZ9UmquXvlc8Jm9k+S9kn6lbtfkG1bKukzd38k+4dztLv/W5f09qCkfVWv3JwtKDNu8MrSkm6QdIsqfO4Sfd2kCp63Ko780yR95O4fu/tBSb+WNLOCPrqeu6+X9Nkxm2dKWpndXqmB/3k6Lqe3ruDuO9393ez255KOrixd6XOX6KsSVYR/vKStg+5vU3ct+e2Sfmdm75jZgqqbGcLp2bLpR5dPP63ifo5VuHJzJx2zsnTXPHfNrHhdtirCP9TqP9005XCFu18k6YeSfpK9vEVjGlq5uVOGWFm6KzS74nXZqgj/NkkTB92fIGlHBX0Myd13ZD93S3pB3bf68K6ji6RmP3dX3M/fdNPKzUOtLK0ueO66acXrKsK/UdI5ZvZ9MxshaY6ktRX08Q1mNjL7IEZmNlLSteq+1YfXSpqX3Z4naU2FvXxNt6zcnLeytCp+7rptxetKvuSTTWX8h6Rhkla4+0Mdb2IIZnaWBo720sAips9V2ZuZPS/pKg2c9bVL0k8lvShplaQzJf1F0mx37/gHbzm9XaWBl65/W7n56HvsDvf2j5LekLRZ0pFs830aeH9d2XOX6GuuKnje+IYfEBTf8AOCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENT/A3I1PoNl2SpuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note that imshow also works fine with scaled\n",
    "# images in [0, 1] range.\n",
    "plt.imshow(one_image.squeeze(), cmap='binary');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
